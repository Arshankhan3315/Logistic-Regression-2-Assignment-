{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef25f27f-4f87-4111-95f2-a86c215273d8",
   "metadata": {},
   "source": [
    "# Logistic Regression-2 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bb7bfc-6306-4c86-8b23-0914f9f9ded3",
   "metadata": {},
   "source": [
    "# Q1. What is the purpose of grid search cv in machine learning, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e734e92-79a2-4ad7-9f47-3a8b489a5926",
   "metadata": {},
   "source": [
    "# Answer-1-Grid Search Cross-Validation (GridSearchCV) is a technique used in machine learning to find the optimal hyperparameters for a model. Hyperparameters are parameters that are not learned from the data but are set prior to training the model. They significantly influence the model's performance and generalization ability.\n",
    "\n",
    "# The purpose of GridSearchCV is to systematically search through a predefined set of hyperparameter combinations, evaluating each combination using cross-validation to determine the combination that yields the best performance. The goal is to find the hyperparameters that lead to the most accurate and well-generalized model.\n",
    "\n",
    "# Here's how GridSearchCV works:\n",
    "\n",
    "# Define the Hyperparameter Grid:\n",
    "\n",
    "- You specify a grid of hyperparameter values that you want to search. For example, if you're tuning the hyperparameters of a support vector machine (SVM), you might define a grid for parameters like the type of kernel, the regularization parameter (C), and the kernel coefficient.\n",
    "# Cross-Validation:\n",
    "\n",
    "- The dataset is split into multiple folds (or subsets). The model is trained on a combination of folds and validated on the remaining one. This process is repeated for each fold, and the average performance across all folds is computed.\n",
    "# Model Training and Evaluation:\n",
    "\n",
    "- For each combination of hyperparameters in the grid, the model is trained using the training data from each fold and evaluated on the corresponding validation set. This is done for all folds, and the average performance is calculated.\n",
    "# Choose the Best Hyperparameters:\n",
    "\n",
    "- The combination of hyperparameters that resulted in the best average performance (according to a specified evaluation metric, e.g., accuracy, F1 score, etc.) is selected as the optimal set of hyperparameters.\n",
    "# Final Model:\n",
    "\n",
    "- Once the best hyperparameters are identified, the final model is trained on the entire dataset using these optimal hyperparameters.\n",
    "- GridSearchCV helps automate and simplify the process of hyperparameter tuning, ensuring that you explore a comprehensive set of hyperparameter combinations without the need for manual tuning. It is an essential tool for optimizing models and improving their predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986e4da3-b2b3-41df-a06f-e51d8f461679",
   "metadata": {},
   "source": [
    "# Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa70f41-5738-4e5e-b505-06ff58b0733a",
   "metadata": {},
   "source": [
    "# Answer-2-Grid Search CV and Randomized Search CV are both techniques used for hyperparameter tuning in machine learning, but they differ in how they explore the hyperparameter space.\n",
    "\n",
    "# Grid Search CV:\n",
    "\n",
    "- Search Strategy: Grid search exhaustively evaluates all possible combinations of hyperparameter values specified in a predefined grid.\n",
    "- Computational Cost: It can be computationally expensive, especially when the hyperparameter space is large or when the dataset is large.\n",
    "- Use Case: Grid search is suitable when you have a relatively small number of hyperparameters to tune, and you have a good understanding of their potential values. It ensures a thorough exploration of the entire search space.\n",
    "# Randomized Search CV:\n",
    "\n",
    "- Search Strategy: Randomized search randomly samples a fixed number of hyperparameter combinations from the specified hyperparameter space. Not all combinations are explored.\n",
    "- Computational Cost: It is computationally more efficient compared to grid search, as it does not exhaustively search the entire space. The number of sampled combinations can be controlled.\n",
    "- Use Case: Randomized search is useful when the hyperparameter space is large and searching all combinations is impractical due to computational constraints. It provides a good balance between exploration and computational efficiency.\n",
    "# When to Choose One Over the Other:\n",
    "\n",
    "# Grid Search:\n",
    "\n",
    "- Use grid search when the hyperparameter space is relatively small.\n",
    "- When computational resources are sufficient to explore all combinations.\n",
    "- If you have a good understanding of the hyperparameter values that are likely to work well.\n",
    "# Randomized Search:\n",
    "\n",
    "- Use randomized search when the hyperparameter space is large and exploring all combinations is not feasible.\n",
    "- When you want to balance computational cost with the quality of the solution.\n",
    "- If you're in the early stages of model development and want to quickly identify a promising set of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524cef61-0f33-418e-813f-76f648faf06e",
   "metadata": {},
   "source": [
    "# Q3. What is data leakage, and why is it a problem in machine learning? Provide an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c5671e-6cdb-42e2-bf62-2f1913b46941",
   "metadata": {},
   "source": [
    "# Answer-3-Data leakage in machine learning occurs when information from outside the training dataset is used to create a model. This additional information can inadvertently provide the model with insights that it would not have during real-world predictions, leading to overly optimistic performance estimates. Data leakage can result in a model that performs well on the training data but fails to generalize to new, unseen data.\n",
    "\n",
    "# Data leakage can take several forms:\n",
    "\n",
    "# Leakage in Training Data:\n",
    "\n",
    "- If information from the test set is accidentally included in the training set, the model may learn patterns that do not generalize well. For example, including future information (data that would not be available at the time of prediction) in the training set can lead to optimistic performance estimates.\n",
    "# Leakage in Features:\n",
    "\n",
    "- If the model is trained on features that are derived from the target variable or contain information about the target variable that would not be available during prediction, it can lead to overfitting. For instance, using a variable that is directly related to the target variable but obtained post-event as a predictor can introduce leakage.\n",
    "# Example of Data Leakage:\n",
    "\n",
    "- Consider a credit scoring model where the goal is to predict whether an individual will default on a loan. The dataset contains information about each individual's financial history, including whether they defaulted on previous loans. The dataset also includes a variable indicating whether the individual has defaulted on the current loan.\n",
    "\n",
    "- In this scenario, a data leakage issue might arise if the model is trained using information about whether an individual has defaulted on the current loan. This is because the model could learn to associate the target variable (loan default) with information that would not be available at the time of making predictions.\n",
    "\n",
    "- To prevent data leakage in this example, it would be important to exclude any information about the current loan status when training the model. The model should only be exposed to information that would realistically be available at the time of predicting loan default for a new, unseen individual.\n",
    "\n",
    "- Detecting and preventing data leakage is crucial for building models that generalize well to new data and make reliable predictions in real-world scenarios. Cross-validation and careful feature engineering are common strategies to help identify and mitigate data leakage issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac41ef90-2328-4c86-bf1b-c2c7db1bc019",
   "metadata": {},
   "source": [
    "# Q4. How can you prevent data leakage when building a machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a799903-16d0-4389-ac02-b83e0ba9c9f1",
   "metadata": {},
   "source": [
    "# Answer-4-Preventing data leakage is crucial for building machine learning models that generalize well to new, unseen data. Here are some strategies to help prevent data leakage:\n",
    "\n",
    "# Separate Training and Test Sets:\n",
    "\n",
    "- Ensure a clear separation between the training set and the test set. Information from the test set should not be used during any stage of model training, including feature selection, hyperparameter tuning, or model fitting.\n",
    "# Use Cross-Validation:\n",
    "\n",
    "- Employ cross-validation techniques (e.g., k-fold cross-validation) when assessing model performance. This helps in evaluating the model's generalization ability across different subsets of the data without leaking information from the test set into the training process.\n",
    "# Feature Engineering with Caution:\n",
    "\n",
    "- Be cautious when creating new features, especially if they involve information that would not be available at the time of prediction. Ensure that feature engineering is performed based only on information available in the training set and does not leak information from the test set.\n",
    "# Time Series Considerations:\n",
    "\n",
    "- When working with time series data, ensure that the training set includes only information available up to a certain point in time. The test set should represent data from a future time to simulate real-world prediction scenarios.\n",
    "# Exclude Future Information:\n",
    "\n",
    "- Avoid including information in the training set that would not be available during prediction. For example, if predicting an event in the future, exclude any data related to that event that occurs after the prediction point.\n",
    "# Remove Redundant Predictors:\n",
    "\n",
    "- Remove predictors that are highly correlated with the target variable or have a direct causal relationship with the target but would not be known at the time of prediction.\n",
    "# Be Mindful of Preprocessing Steps:\n",
    "\n",
    "- If using preprocessing techniques such as imputation or normalization, ensure that these steps are performed separately on the training and test sets. Information from the test set should not be used to inform preprocessing decisions for the training set.\n",
    "# Understand the Data Source:\n",
    "\n",
    "- Have a thorough understanding of the data source and the domain to identify potential sources of leakage. This understanding can help you make informed decisions about which features to include and how to preprocess the data.\n",
    "# Document Steps and Decisions:\n",
    "\n",
    "- Keep detailed documentation of the preprocessing steps, feature engineering, and modeling decisions. This documentation helps in identifying and addressing potential sources of data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80cfeb1-9a9d-4d90-b904-d435875aaec5",
   "metadata": {},
   "source": [
    "# Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329bd7b5-c332-4309-8ac7-32b364ed677f",
   "metadata": {},
   "source": [
    "# Answer-5-A confusion matrix is a table that is used to evaluate the performance of a classification model on a set of data for which the true values are known. It provides a detailed breakdown of the model's predictions and how they compare to the actual outcomes. The confusion matrix is particularly useful in binary classification problems, where there are two classes (e.g., positive and negative).\n",
    "\n",
    "# Here are the key components of a confusion matrix:\n",
    "\n",
    "# True Positive (TP):\n",
    "\n",
    "- Instances where the model correctly predicts the positive class. For example, correctly identifying individuals with a disease.\n",
    "# True Negative (TN):\n",
    "\n",
    "- Instances where the model correctly predicts the negative class. For example, correctly identifying individuals without a disease.\n",
    "# False Positive (FP):\n",
    "\n",
    "- Instances where the model incorrectly predicts the positive class. Also known as a Type I error. For example, predicting someone has a disease when they do not.\n",
    "# False Negative (FN):\n",
    "\n",
    "- Instances where the model incorrectly predicts the negative class. Also known as a Type II error. For example, failing to predict someone has a disease when they do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c3fe47-9950-4ff0-b3fd-c2e40cad4746",
   "metadata": {},
   "source": [
    "# Accuracy:\n",
    "\n",
    "- The overall correctness of the model, calculated as (TP + TN) / (TP + TN + FP + FN).\n",
    "# Precision (Positive Predictive Value):\n",
    "\n",
    "- The accuracy of positive predictions, calculated as TP / (TP + FP). Precision focuses on the proportion of positive predictions that are actually correct.\n",
    "# Recall (Sensitivity, True Positive Rate):\n",
    "\n",
    "- The ability of the model to capture all positive instances, calculated as TP / (TP + FN). Recall focuses on the proportion of actual positives that are correctly identified by the model.\n",
    "# Specificity (True Negative Rate):\n",
    "\n",
    "- The ability of the model to correctly identify negative instances, calculated as TN / (TN + FP).\n",
    "# F1 Score:\n",
    "\n",
    "- The harmonic mean of precision and recall, calculated as 2 * (Precision * Recall) / (Precision + Recall). It provides a balance between precision and recall.\n",
    "# False Positive Rate (FPR):\n",
    "\n",
    "- The proportion of actual negatives that are incorrectly predicted as positives, calculated as FP / (TN + FP).\n",
    "- The confusion matrix and associated metrics provide a more detailed and nuanced understanding of a classification model's performance than accuracy alone. This breakdown is especially valuable when dealing with imbalanced datasets or when different types of errors have varying consequences in a specific application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a5d900-d6dd-4c6b-8cc8-cfe40f2a027e",
   "metadata": {},
   "source": [
    "# Q6. Explain the difference between precision and recall in the context of a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecab0de-4957-4116-971d-d85907980339",
   "metadata": {},
   "source": [
    "# Answer-6-Precision and recall are two important performance metrics derived from a confusion matrix, and they provide insights into different aspects of a classification model's performance.\n",
    "\n",
    "# Precision:\n",
    "\n",
    "- Formula: Precision is calculated as TP / (TP + FP), where TP is the number of true positives, and FP is the number of false positives.\n",
    "- Interpretation: Precision represents the accuracy of positive predictions made by the model. It answers the question: \"Of all the instances predicted as positive, how many were truly positive?\"\n",
    "- Focus: Precision is particularly relevant in situations where the cost of false positives is high. For example, in medical diagnoses, a high precision indicates that when the model predicts a positive result, it is likely correct.\n",
    "# Recall (Sensitivity, True Positive Rate):\n",
    "\n",
    "- Formula: Recall is calculated as TP / (TP + FN), where TP is the number of true positives, and FN is the number of false negatives.\n",
    "- Interpretation: Recall measures the ability of the model to capture all positive instances. It answers the question: \"Of all the instances that were truly positive, how many did the model correctly identify?\"\n",
    "- Focus: Recall is particularly relevant in situations where missing positive instances (false negatives) is costly or has severe consequences. For example, in fraud detection, a high recall indicates that the model is effective at capturing most fraudulent transactions.\n",
    "# Key Differences:\n",
    "\n",
    "- Precision is concerned with the accuracy of positive predictions relative to all positive predictions made by the model. It's a measure of how well the model avoids false positives.\n",
    "\n",
    "- Recall is concerned with the ability of the model to capture all positive instances, and it focuses on avoiding false negatives.\n",
    "\n",
    "- In practice, there is often a trade-off between precision and recall. Increasing one may lead to a decrease in the other. The balance between precision and recall is often assessed using metrics like the F1 score, which is the harmonic mean of precision and recall. The choice between optimizing for precision or recall depends on the specific requirements and constraints of the application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67554619-b457-47d7-b1b6-217aa1501980",
   "metadata": {},
   "source": [
    "# Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a0b0b8-ed08-4de3-8d9c-78b1add19657",
   "metadata": {},
   "source": [
    "# Answer-7-Interpreting a confusion matrix involves understanding the different components of the matrix to gain insights into the types of errors your model is making. The key elements of a confusion matrix are True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN). These elements provide information about the model's predictions and their alignment with the actual outcomes. Here's how you can interpret a confusion matrix:\n",
    "\n",
    "# True Positives (TP):\n",
    "\n",
    "- Interpretation: Instances that were correctly predicted as positive by the model.\n",
    "- Significance: A high TP count indicates that the model is effective at identifying positive cases.\n",
    "# True Negatives (TN):\n",
    "\n",
    "- Interpretation: Instances that were correctly predicted as negative by the model.\n",
    "- Significance: A high TN count indicates that the model is effective at identifying negative cases.\n",
    "# False Positives (FP):\n",
    "\n",
    "- Interpretation: Instances that were incorrectly predicted as positive by the model (Type I errors).\n",
    "- Significance: FP errors represent instances where the model falsely indicated the presence of a positive condition. Investigate the causes of these errors and assess the impact on the specific application.\n",
    "# False Negatives (FN):\n",
    "\n",
    "- Interpretation: Instances that were incorrectly predicted as negative by the model (Type II errors).\n",
    "- Significance: FN errors represent instances where the model failed to detect a positive condition. Investigate the causes of these errors, especially if missing positive instances has significant consequences.\n",
    "- Using these elements, you can derive various performance metrics and gain a deeper understanding of your model's strengths and weaknesses:\n",
    "\n",
    "- Precision: Focus on FP errors. If precision is low, the model is making a significant number of false positive predictions.\n",
    "\n",
    "- Recall: Focus on FN errors. If recall is low, the model is missing a significant number of positive instances.\n",
    "\n",
    "- Accuracy: Assess overall correctness. A low accuracy may indicate that the model is struggling to correctly predict both positive and negative instances.\n",
    "\n",
    "- F1 Score: Consider the balance between precision and recall. The F1 score is particularly useful when there is a need to balance false positives and false negatives.\n",
    "\n",
    "- False Positive Rate (FPR): Focus on FP errors relative to the actual negatives. If FPR is high, the model is incorrectly predicting positives among the negatives.\n",
    "\n",
    "- Interpreting a confusion matrix is an essential step in model evaluation and can guide further improvements or adjustments to address specific types of errors that have practical implications in the application domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8cfea4-3f0c-4962-914a-f6d328ffdbc0",
   "metadata": {},
   "source": [
    "# Q8. What are some common metrics that can be derived from a confusion matrix, and how are they calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e0aedb-8eb9-4ea0-af9d-cf84048631eb",
   "metadata": {},
   "source": [
    "# Answer-8-Several common metrics can be derived from a confusion matrix to evaluate the performance of a classification model. These metrics provide insights into different aspects of the model's behavior, such as accuracy, precision, recall, and F1 score. Here are some common metrics and their formulas:\n",
    "\n",
    "# Accuracy:\n",
    "\n",
    "- Formula: Accuracy =TP+TN/TP+TN+FP+FN\n",
    "- Interpretation: The proportion of correctly classified instances among all instances.\n",
    "# Precision (Positive Predictive Value):\n",
    "\n",
    "- Formula: Precision=TP/TP+FP \n",
    "- Interpretation: The accuracy of positive predictions. It answers the question: \"Of all the instances predicted as positive, how many were truly positive?\"\n",
    "# Recall (Sensitivity, True Positive Rate):\n",
    "\n",
    "- Formula: Recall=TP/TP+FN\n",
    "- Interpretation: The ability of the model to capture all positive instances. It answers the question: \"Of all the instances that were truly positive, how many did the model correctly identify?\"\n",
    "# Specificity (True Negative Rate):\n",
    "\n",
    "- Formula: Specificity=TN/TN+FP\n",
    "- Interpretation: The ability of the model to correctly identify negative instances.\n",
    "# F1 Score:\n",
    "\n",
    "- Formula: F1 Score=2×Precision×Recall/Precision+Recall\n",
    "- Interpretation: The harmonic mean of precision and recall. It provides a balance between precision and recall.\n",
    "# False Positive Rate (FPR):\n",
    "\n",
    "- Formula: FPR=FP/TN+FP\n",
    "- Interpretation: The proportion of actual negatives that are incorrectly predicted as positives.\n",
    "# These metrics are derived from the values in a confusion matrix, where:\n",
    "\n",
    "- TP is the number of true positives,\n",
    "- TN is the number of true negatives,\n",
    "- FP is the number of false positives, and\n",
    "- FN is the number of false negatives.\n",
    "- By considering these metrics, you can gain a comprehensive understanding of how well your model is performing and identify areas for improvement based on the specific goals and requirements of your application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161d686e-b470-4f07-90f5-b1d8e85777ba",
   "metadata": {},
   "source": [
    "# Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d32cc9a-f8a4-4ce3-9638-dbb2f69df805",
   "metadata": {},
   "source": [
    "# Answer-9-The accuracy of a model is directly related to the values in its confusion matrix. Accuracy is a metric that measures the overall correctness of the model's predictions. It is calculated as the ratio of the sum of true positives (TP) and true negatives (TN) to the total number of instances. The relationship between accuracy and the confusion matrix elements can be expressed as follows:\n",
    "\n",
    "# Accuracy:\n",
    "- Accuracy=TP+TN/TP+TN+FP+FN\n",
    "\n",
    "# Here's how accuracy is related to the confusion matrix components:\n",
    "\n",
    "# True Positives (TP):\n",
    "\n",
    "- Instances correctly predicted as positive.\n",
    "# True Negatives (TN):\n",
    "\n",
    "- Instances correctly predicted as negative.\n",
    "# False Positives (FP):\n",
    "\n",
    "- Instances incorrectly predicted as positive.\n",
    "# False Negatives (FN):\n",
    "\n",
    "- Instances incorrectly predicted as negative.\n",
    "- Accuracy considers both positive and negative predictions, and it rewards the model for making correct predictions in both classes. It is a general metric that provides an overall assessment of the model's performance across all classes.\n",
    "\n",
    "- However, it's essential to be cautious when relying solely on accuracy, especially in imbalanced datasets where one class is more prevalent than the other. In such cases, a model might achieve high accuracy by predominantly predicting the majority class, while performance on the minority class may be poor. This is why precision, recall, F1 score, and other metrics derived from the confusion matrix are often used in conjunction with accuracy to provide a more nuanced evaluation of a model's performance, particularly when class distribution is uneven."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60c54e0-696a-4070-8d46-e1823a892905",
   "metadata": {},
   "source": [
    "# Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc90103f-bb27-4082-b13c-3956bc865963",
   "metadata": {},
   "source": [
    "# Answer-7-A confusion matrix can be a valuable tool for identifying potential biases or limitations in your machine learning model, particularly with respect to how it performs across different classes. Here are some ways to leverage a confusion matrix for this purpose:\n",
    "\n",
    "# Class Imbalance:\n",
    "\n",
    "- Observation: Check for significant differences in the number of instances between classes (positive and negative).\n",
    "- Implication: If there is a severe class imbalance, the model might be biased towards the majority class. High accuracy might not reflect the model's effectiveness in predicting the minority class.\n",
    "# False Positives and False Negatives:\n",
    "\n",
    "- Observation: Examine the counts of false positives (FP) and false negatives (FN) for each class.\n",
    "- Implication: A disproportionate number of false positives or false negatives for a specific class may indicate biases or limitations in the model's ability to correctly predict instances of that class.\n",
    "# Precision and Recall Disparities:\n",
    "\n",
    "- Observation: Compare precision and recall values across different classes.\n",
    "- Implication: Significant differences in precision or recall between classes may highlight biases. For example, if recall is low for a certain class, the model is not effectively capturing instances of that class.\n",
    "# Confusion between Similar Classes:\n",
    "\n",
    "- Observation: Analyze instances of confusion between classes that are similar or easily confused.\n",
    "- Implication: If the model consistently misclassifies instances between similar classes, it may indicate that the features used for prediction are not capturing the nuances that differentiate these classes.\n",
    "# Review Misclassified Instances:\n",
    "\n",
    "- Observation: Examine specific instances that are frequently misclassified.\n",
    "- Implication: Understanding the characteristics of instances that the model struggles to predict can provide insights into the limitations of the features or the model architecture.\n",
    "# Threshold Adjustments:\n",
    "\n",
    "- Observation: Evaluate the impact of adjusting classification thresholds.\n",
    "- Implication: Adjusting the threshold for classification can influence the balance between precision and recall. Understanding the trade-offs and choosing an appropriate threshold may address biases or limitations in the model.\n",
    "# Domain Knowledge:\n",
    "\n",
    "- Observation: Apply domain knowledge to interpret confusion matrix results.\n",
    "- Implication: Incorporating domain expertise allows you to identify whether misclassifications are due to genuine difficulties in prediction or if there are underlying issues in the data or modeling process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4116feff-a3ab-4c17-97c1-3cd3f3bfab27",
   "metadata": {},
   "source": [
    "# Completed Assignment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
